\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{indentfirst}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in
\begin{document}

% ========== Edit your name here
\author{Chengze(Kol) Zuo\\cz1565@nyu.edu}
\title{DS-GA 1008 Homework Assignment 1}
\date{February 14, 2019}
\maketitle

% ========== Begin answering questions here

\section*{\centering Part I}
\subsection*{\indent1.1}

According to the instructions, let \textbf{x} be the column vector
$\left[
\begin{matrix}
x_{1}\\
x_{2}
\end{matrix}
\right]$, let \textbf{y} be the column vector
$\left[
\begin{matrix}
y_{1}\\
y_{2}
\end{matrix}
\right]$, and let the weight matrix \textbf{W} be the 2x2 matrix
$\left[
\begin{matrix}
w_{1,1} & w_{1,2}\\
x_{2,1} & w_{2,2}
\end{matrix}
\right]$. Applying the $\textbf{y} = \textbf{W}\textbf{x} + \textbf{b}$ can give us $\textbf{y} = 
\left[\begin{matrix}
w_{1,1}x_{1}+w_{1,2}x_{2}+b_{1}\\
w_{2,1}x_{1}+w_{2,2}x_{2}+b_{2}
\end{matrix}\right]$. For the $\frac{\partial L}{\partial\textbf{W}}$, its shape would be same as the \textbf{W}, and $\frac{\partial L}{\partial\textbf{W}} = 
\left[\begin{matrix}
\frac{\partial L}{\partial w_{1,1}} & \frac{\partial L}{\partial w_{1,2}}\\
\frac{\partial L}{\partial w_{2,1}} & \frac{\partial L}{\partial w_{2,2}}
\end{matrix}\right]$. For now, we focus on solving $\frac{\partial L}{\partial w_{1,1}}$, and then apply the same approach to $\frac{\partial L}{\partial w_{1,2}}$, $\frac{\partial L}{\partial w_{2,1}}$, etc.\setstretch{1.5}According to the chain rule, we can infer that:
\begin{center}
    $\frac{\partial L}{\partial w_{1,1}} = \frac{\partial L}{\partial \textbf{y}}\cdot\frac{\partial\textbf{y}}{\partial w_{1,1}}$
\end{center}, where:
\begin{center}
    $
    \frac{\partial L}{\partial \textbf{y}} = 
    \left[\begin{matrix}
    \frac{\partial L}{\partial y_{1}}\\
    \frac{\partial L}{\partial y_{2}}
    \end{matrix}\right] = 
    \left[\begin{matrix}
    dy_{1}\\
    dy_{2}
    \end{matrix}\right]$
\end{center}, and $\frac{\partial\textbf{y}}{\partial w_{1,1}}$ equals to:
\begin{center}
    $
    \frac{\partial\textbf{y}}{\partial w_{1,1}}=
    \left[\begin{matrix}
    \frac{\partial y_{1}}{\partial w_{1,1}}\\
    \frac{\partial y_{2}}{\partial w_{1,1}}
    \end{matrix}\right]=
    \left[\begin{matrix}
    x_{1}\\
    0
    \end{matrix}\right]
    $
\end{center}Therefore, after applying the dot product between two vectors, we can get:
\begin{center}
    $
    \frac{\partial  L}{\partial w_{1,1}} = 
    dy_{1}x_{1}
    $
\end{center}Therefore, we can infer that:
\begin{center}
    $\frac{\partial L}{\partial\textbf{W}} = 
    \left[\begin{matrix}
    \frac{\partial L}{\partial w_{1,1}} & \frac{\partial L}{\partial w_{1,2}}\\
    \frac{\partial L}{\partial w_{2,1}} & \frac{\partial L}{\partial w_{2,2}}
    \end{matrix}\right]=
    \left[\begin{matrix}
    dy_{1}x_{1} & dy_{1}x_{2}\\
    dy_{2}x_{1} & dy_{2}x_{2}
    \end{matrix}\right]=
    \left[\begin{matrix}
    dy_{1}\\
    dy_{2}
    \end{matrix}\right]
    \left[\begin{matrix}
    x_{1} & x_{2}
    \end{matrix}\right]=
    \frac{\partial L}{\partial\textbf{y}}\mathbf{x}^T
    $
\end{center}Using the same approach, for the $\frac{\partial L}{\partial \mathbf{b}}$, we can infer that:
\begin{center}
    $\frac{\partial L}{\partial \mathbf{b}} = \frac{\partial L}{\partial \textbf{y}}\cdot\frac{\partial\mathbf{y}}{\partial \mathbf{b}}$
\end{center}where:
\begin{center}
    $
    \frac{\partial\mathbf{L}}{\partial \mathbf{b}}=
    \left[\begin{matrix}
    \frac{\partial\mathbf{L}}{\partial b_{1}} \\
    \frac{\partial\mathbf{L}}{\partial b_{2}} 
    \end{matrix}\right]
    $
\end{center}    
\begin{center}    
    $
    \frac{\partial\textbf{y}}{\partial b_{1}}=
    \left[\begin{matrix}
    \frac{\partial y_{1}}{\partial b_{1}} \\
    \frac{\partial y_{2}}{\partial b_{1}} 
    \end{matrix}\right]=
    \left[\begin{matrix}
    1\\
    0
    \end{matrix}\right]
    $
\end{center}Therefore, $\frac{\partial L}{\partial b_{1}}$ would be:
\begin{center}
    $
    \frac{\partial L}{\partial b_{1}}=
    \left[\begin{matrix}
    dy_{1} \\
    dy_{2}
    \end{matrix}\right]\cdot\left[\begin{matrix}
    1\\
    0
    \end{matrix}\right] = dy_{1}
    $
\end{center}Also, $\frac{\partial L}{\partial b_{2}}$ would be $dy_{2}$. Thus: 
\begin{center}
    $
    \frac{\partial L}{\partial \mathbf{b}}= 
    \left[\begin{matrix}
    dy_{1} \\
    dy_{2}
    \end{matrix}\right]=
    \frac{\partial\mathbf{L}}{\partial\mathbf{y}}
    $
    \begin{center}
        (i.e., $\mathbf{I}\frac{\partial\mathbf{L}}{\partial\mathbf{y}}$, the Jacobian matrix $\frac{\partial\mathbf{y}}{\partial\mathbf{b}}$ is an Identity matrix)
    \end{center}
\end{center}


\subsection*{\indent1.2}

The softmax expression which indicates the probability of the j-th class is as follows:
\begin{center}
    $P(z=j|\mathbf{x}) = y_{i} = \frac{e^{\beta x_{j}}}{\sum_{i}e^{\beta x_{i}}}$
\end{center}To compute the Jacobian matrix $\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ looks like:
\begin{center}
    $
    \frac{\partial \mathbf{y}}{\partial \mathbf{x}}=
        \left[\begin{matrix}
        \frac{\partial y_{1}}{\partial x_{1}} & \dots & \frac{\partial y_{1}}{\partial x_{n}} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial y_{n}}{\partial x_{1}} & \dots & \frac{\partial y_{n}}{\partial x_{n}}
        \end{matrix}\right]
    $
\end{center}\setstretch{1.5}For computing any given partial derivative, i.e., the rate of the change for $j$-th element $y_{j}$ in the output vector w.r.t $i$-th element $x_{i}$ in the input vector, the $\frac{\partial y_{j}}{\partial x_{i}}$ would be computed as follow:
\begin{center}
    $
    \frac{\partial y_{j}}{\partial x_{i}}=
    \frac{\partial \frac{e^{\beta x_{j}}}{\sum_{i}e^{\beta x_{i}}}}{\partial x_{i}}
    $
\end{center}Given the quotient rule for computing derivatives, suppose:
\begin{center}
    $
    f(x) = \frac{h(x)}{g(x)}
    $
\end{center}we know that:
\begin{center}
    $
    f'(x) = \frac{h'(x)g(x)-g'(x)h(x)}{[g(x)]^2}
    $
\end{center}For now, $h_i = e^{\beta x_{i}}$, and $g_i = \sum_{i}e^{\beta x_{i}}$. For the convenience, first we simply use the summation symbol $\sum$ to represent $\sum_{i}e^{\beta x_{i}}$. Note that no matter what $x_{i}$ we compute the derivative of $g(x)$, the result would always be $\beta e^{\beta x_{i}}$. However for computing the derivative of $e^{\beta x_{j}}$ w.r.t $x_{i}$, if i = j, the result would be $\beta e^{\beta x_{i}}$, otherwise it would be zero.
\newline\indent Therefore, if i = j:
\begin{center}
    $
    \frac{\partial y_{j}}{\partial x_{i}}=
    \frac{\partial \frac{e^{\beta x_{j}}}{\sum_{i}e^{\beta x_{i}}}}{\partial x_{i}}=
    \frac{\beta e^{\beta x_{i}}\sum - e^{\beta x_{j}}\beta e^{\beta x_{i}}}{\sum^2}
    $\\
    $
    =\frac{\beta e^{\beta x_{i}}}{\sum}\frac{\sum - e^{\beta x_{j}}}{\sum}
    $\\
    $
    = \beta y_{i}(1-y_{j})
    $
\end{center}or if i $\neq$ j:
\begin{center}
    $
     \frac{\partial y_{j}}{\partial x_{i}}=
    \frac{\partial \frac{e^{\beta x_{j}}}{\sum_{i}e^{\beta x_{i}}}}{\partial x_{i}}=
    \frac{0 - e^{\beta x_{j}}\beta e^{\beta x_{i}}}{\sum^2}  
    $\\
    $
    = -\frac{e^{\beta x_{j}}}{\sum}\frac{\beta e^{\beta x_{i}}}{\sum}
    $\\
    $
    = -y_{j}\beta y_{i}
    $\\
    $
    = -\beta y_{j}y_{i}
    $
\end{center}To conclude, the expression for $\frac{\partial y_{j}}{\partial x{i}}$ would be:
\begin{center}
    $
    \frac{\partial y_{j}}{\partial x{i}}=
    \begin{cases}
        \beta y_{i}(1-y_{j}) & \text i = j \\
        -\beta y_{j}y_{i} & \text i \neq j
    \end{cases}
    $
\end{center}Or we can formulate it more concisely:
\begin{center}
    $
        \frac{\partial y_{j}}{\partial x{i}}=
        \beta y_{i}(1(i = j) - y_{j})
    $
\end{center}





\end{document}
\grid
\grid